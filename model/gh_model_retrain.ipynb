{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c35d1cc",
   "metadata": {},
   "source": [
    "# Greenhouse Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195db0c9",
   "metadata": {},
   "source": [
    "## Import dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77453a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5dee1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant paths\n",
    "home_path = os.path.dirname(os.getcwd())\n",
    "data_path = home_path + '\\\\data\\\\'\n",
    "plot_path = home_path + '\\\\plotting\\\\plots\\\\'\n",
    "save_path = home_path + '\\\\model\\\\saved\\\\'\n",
    "results_path = home_path + '\\\\model\\\\results\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a747b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get merged data\n",
    "data = pd.read_csv(\n",
    "    data_path + 'data_processed.csv',\n",
    "    header=[0, 1],\n",
    "    index_col=[0, 1, 2, 3, 4]\n",
    ")\n",
    "\n",
    "# convert index.date col to datetime\n",
    "#data.index = pd.to_datetime(data.index.values)\n",
    "#data.loc[:, ('time', 'date')] = pd.to_datetime(data.time.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be306b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NAs\n",
      "\n",
      "category  sensor_ID              \n",
      "flow      TA01_GP101                 0\n",
      "          TA02_GP101                 0\n",
      "state     TA01_output                0\n",
      "          TA02_output                0\n",
      "power     phase                      0\n",
      "                                    ..\n",
      "humidity  TA01_GT401_GM401_scaled    0\n",
      "          TA02_GT401_GM401_scaled    0\n",
      "          outdoor_scaled             0\n",
      "          TA_inflow                  0\n",
      "          TA_inflow_out              0\n",
      "Length: 68, dtype: int64\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print number of NAs\n",
    "print('Number of NAs\\n')\n",
    "print(data.isna().sum())\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe10b4",
   "metadata": {},
   "source": [
    "## MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8f10ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_humid(temp, rel_humid):\n",
    "    abs_humidity =  6.112 * np.exp(17.67 * temp / (temp + 243.5)) * rel_humid * 2.1674 / (273.15 + temp)\n",
    "    return abs_humidity\n",
    "\n",
    "def rel_humid(temp, abs_humid):\n",
    "    rel_humidity = abs_humid * (273.15 + temp) / (6.112 * np.exp(17.67 * temp / (temp + 243.5)) * 2.1674)\n",
    "    return rel_humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24579a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta01_min = 35\n",
    "ta01_max = 75\n",
    "\n",
    "ta02_min = 0\n",
    "ta02_max = 85"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ace44",
   "metadata": {},
   "source": [
    "## Split TRAIN / TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85f830c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select days for trainin and testing\n",
    "days = data.groupby(['month', 'day'], sort=False).count().index.values\n",
    "\n",
    "# get number of days f. testing / training\n",
    "train_frac = 0.95\n",
    "train_n = int(len(days) * train_frac)\n",
    "test_n = len(days) - train_n\n",
    "\n",
    "# split dataset\n",
    "#np.random.shuffle(days)\n",
    "mask = np.array([day in list(days[:train_n]) for day in data.index.droplevel(-1).droplevel(-1).droplevel(-1).values])\n",
    "data_train = data.loc[mask].copy()\n",
    "data_test = data.loc[~mask].copy()\n",
    "\n",
    "# del data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4015f82a",
   "metadata": {},
   "source": [
    "### Reduce to M-min observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad9bd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set min per observation\n",
    "m = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3ab8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # reduce to one OBS per 5 MIN (take average of 10 OBS)\n",
    "def data_reduce(data, m):\n",
    "    idxObj = zip(\n",
    "        data.index.get_level_values(0),\n",
    "        data.index.get_level_values(1),\n",
    "        data.index.get_level_values(2),\n",
    "        data.index.get_level_values(3) // m\n",
    "    )\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(\n",
    "        tuples=idxObj,\n",
    "        names=['month', 'day', 'hour', 'minute']\n",
    "    )\n",
    "\n",
    "    data.index = index\n",
    "    data = data.groupby(['month', 'day', 'hour', 'minute'], sort=False).mean()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3097b392",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_reduce(data_train, m)\n",
    "data_test = data_reduce(data_test, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cd4b2",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17b6ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model variables\n",
    "model_vars = [\n",
    "#     ('temperatures', 'TA_inflow'),\n",
    "#     ('humidity', 'TA_inflow'),\n",
    "#     ('temperatures', 'TA_inflow_out'),\n",
    "#     ('humidity', 'TA_inflow_out'),\n",
    "#     ('state', 'TA01_output'),\n",
    "#     ('state', 'TA02_output'),\n",
    "#     ('flow', 'TA01_GP101'),\n",
    "#     ('flow', 'TA02_GP101'),\n",
    "#     ('temperatures', 'DC_GT102_GM102'),\n",
    "#     ('temperatures', 'DC_GT103_GM103'),\n",
    "#     ('temperatures', 'DC_GT104_GM104'),\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('temperatures', 'TA01_GT401_GM401_scaled'),\n",
    "    ('temperatures', 'TA02_GT401_GM401_scaled'),\n",
    "    ('temperatures', 'DC_GT301_damped_scaled'),\n",
    "#     ('temperatures', 'DC_GT301_outdoor_scaled'),\n",
    "#     ('temperatures', 'DC_GT401_GM401'),\n",
    "    ('temperatures', 'TA01_GT401_GM401'),\n",
    "#     ('temperatures', 'TA02_GT401_GM401'),\n",
    "#     ('temperatures', 'DC_GT301_damped'),\n",
    "    ('temperatures', 'DC_GT301_outdoor'),\n",
    "#     ('humidity', 'TA01_GT10X_GM10X'),\n",
    "    ('humidity', 'TA01_GT10X_GM10X_abs'),\n",
    "    ('humidity', 'TA01_GT401_GM401_abs'),\n",
    "    ('humidity', 'TA02_GT401_GM401_abs'),\n",
    "    ('humidity', 'outdoor_abs'),\n",
    "    ('humidity', 'TA01_GT401_GM401_scaled'),\n",
    "#     ('humidity', 'TA02_GT401_GM401_scaled'),\n",
    "    ('humidity', 'outdoor_scaled'),\n",
    "    ('sun', 'gsi'),\n",
    "#     ('sun', 'gsi_deriv'),\n",
    "#     ('sun', 'vol'),\n",
    "#     ('sun', 'vol_deriv'),\n",
    "#     ('wind', 'Wx'),\n",
    "#     ('wind', 'Wy'),\n",
    "    ('power', 'phase'),\n",
    "    ('time', 'minofday'),\n",
    "    ('time', 'dayofyear')\n",
    "]\n",
    "\n",
    "# filter data\n",
    "data_train = data_train[model_vars].copy()\n",
    "data_test = data_test[model_vars].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b572ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_params = {}\n",
    "for col in data_train.columns:\n",
    "#     cat, var = col\n",
    "#     if cat == 'temperatures' and var == 'TA01_GT10X_GM10X':\n",
    "#         continue\n",
    "    \n",
    "    min_val = data_train[col].min()\n",
    "    max_val = data_train[col].max()\n",
    "    \n",
    "    # normalize\n",
    "    mean = data_train[col].mean()\n",
    "    std = data_train[col].std()\n",
    "    \n",
    "    data_train[col] = (data_train[col] - mean) / std\n",
    "    data_test[col] = (data_test[col] - mean) / std\n",
    "    \n",
    "    col_params[col] = {\n",
    "        'mean':mean,\n",
    "        'std':std,\n",
    "        'max':max_val,\n",
    "        'min':min_val\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70925b7a",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97925877",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_steps = 3    # 5 x 2 = 15-min predictions\n",
    "n_steps = 12    # 5 x 12 = 60-min backwards look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3389acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_train = pd.concat([data_train.pop(col) for col in [\n",
    "#     ('temperatures', 'DC_GT102_GM102'),\n",
    "#     ('temperatures', 'DC_GT103_GM103'),\n",
    "#     ('temperatures', 'DC_GT104_GM104'),\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('humidity', 'TA01_GT10X_GM10X_abs')\n",
    "]], axis=1)\n",
    "\n",
    "targets_test = pd.concat([data_test.pop(col) for col in [\n",
    "#     ('temperatures', 'DC_GT102_GM102'),\n",
    "#     ('temperatures', 'DC_GT103_GM103'),\n",
    "#     ('temperatures', 'DC_GT104_GM104'),\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('humidity', 'TA01_GT10X_GM10X_abs')\n",
    "]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9f6d9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_train = targets_train.copy()[n_steps-t_steps:-t_steps]#.values\n",
    "temps_test = targets_test.copy()[n_steps-t_steps:-t_steps]#.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf0068",
   "metadata": {},
   "source": [
    "### Create sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db044685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_flagger(data, n_steps):\n",
    "    # create flag for erroneous sequences\n",
    "    data['hour'] = data.index.get_level_values(2).values    \n",
    "    data['date_flag'] = data.hour - data.hour.shift(n_steps) > 1\n",
    "    \n",
    "    # get positions in data, w.r.t. n_step removed observations at start\n",
    "    flagged_idx = np.where(data.date_flag.values == 1)\n",
    "    flagged_idx = flagged_idx[0] - n_steps\n",
    "    \n",
    "    del data['hour'], data['date_flag']\n",
    "    \n",
    "    return flagged_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30322ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_maker(data, targets, temps, n_steps):\n",
    "    \n",
    "    vals = data.values\n",
    "    sequences = []\n",
    "    for i in range(len(vals) - n_steps):\n",
    "        sequences.append(vals[i:i+n_steps])\n",
    "    sequences = np.stack(sequences)\n",
    "    \n",
    "    flags = date_flagger(data, n_steps)\n",
    "    mask = [idx not in flags for idx in range(len(sequences))]\n",
    "    \n",
    "    sequences = sequences[mask]\n",
    "    targets = targets[n_steps:][mask].values\n",
    "    temps = temps[mask].values # TEMPORARY\n",
    "    \n",
    "    return sequences, targets, temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ac64bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sequences\n",
    "sequences_train, targets_train, temps_train = seq_maker(data_train, targets_train, temps_train, n_steps)\n",
    "sequences_test, targets_test, temps_test = seq_maker(data_test, targets_test, temps_test, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67a06752",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_masked = sequences_test.copy()\n",
    "\n",
    "mask_len = t_steps\n",
    "for t in range(1, mask_len):\n",
    "    sequences_masked[:, -t, :] = sequences_masked[:, -(t_steps), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9973d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle training data randomly\n",
    "idxs = np.arange(len(targets_train))\n",
    "np.random.shuffle(idxs)\n",
    "\n",
    "sequences_train = sequences_train[idxs]\n",
    "targets_train = targets_train[idxs]\n",
    "temps_train = temps_train[idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaeef6e",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50b88b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gh_lstm_w128_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9b7b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b1f9b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load = tf.keras.models.load_model(save_path + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "117a1659",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(sequences_train.shape[1], sequences_train.shape[2]))\n",
    "inputs = layers.BatchNormalization()(inputs)\n",
    "\n",
    "_, encoder, _ = layers.LSTM(units=128, recurrent_dropout=0.2, return_sequences=False, return_state=True)(inputs)\n",
    "encoder = layers.BatchNormalization()(encoder)\n",
    "encoder = layers.Dropout(0.2)(encoder)\n",
    "\n",
    "temp_input = layers.Input(shape=(2,))\n",
    "temp = layers.GaussianNoise(stddev=1.0)(temp_input)\n",
    "temp = layers.BatchNormalization()(temp)\n",
    "\n",
    "output = layers.Concatenate()([encoder, temp])\n",
    "output = layers.Dense(units=2, activation=None, use_bias=False)(output)\n",
    "\n",
    "encoded = Model(inputs, encoder)\n",
    "model = Model([inputs, temp_input], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0615a63d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 12, 14)]     0           []                               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 128),        73216       ['input_4[0][0]']                \n",
      "                                 (None, 128),                                                     \n",
      "                                 (None, 128)]                                                     \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 2)]          0           []                               \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 128)         512         ['lstm[2][1]']                   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " gaussian_noise (GaussianNoise)  (None, 2)           0           ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 128)          0           ['batch_normalization_1[2][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 2)           8           ['gaussian_noise[1][0]']         \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 130)          0           ['dropout[2][0]',                \n",
      "                                                                  'batch_normalization_2[1][0]']  \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 2)            260         ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 73,996\n",
      "Trainable params: 73,736\n",
      "Non-trainable params: 260\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lr = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "# model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=lr), loss='mse')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eefc08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer, layer_loaded in zip(model.layers, model_load.layers):\n",
    "    layer.set_weights(layer_loaded.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cfecf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # copy weigths from pre-trained\n",
    "# for idx, layer in enumerate(model.layers):\n",
    "#     layer.set_weights(model2.layers[idx].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f7baea1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 9s 29ms/step - loss: 0.0663\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 8s 31ms/step - loss: 0.0428\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.0407\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.0386\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 8s 31ms/step - loss: 0.0354\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.0365\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 7s 29ms/step - loss: 0.0352\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 0.0347\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 0.0332\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 7s 28ms/step - loss: 0.0340\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "trainHist = model.fit(\n",
    "    [sequences_train, temps_train],\n",
    "    targets_train,\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    validation_split=0.0,\n",
    "    callbacks=[\n",
    "#         tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\", restore_best_weights=False),\n",
    "#         tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr * (0.9 ** epoch))\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "48e01c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\\batch_normalization\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\batch_normalization_1\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      ".........3\n",
      "...layers\\concatenate\n",
      "......vars\n",
      "...layers\\dense\n",
      "......vars\n",
      ".........0\n",
      "...layers\\dropout\n",
      "......vars\n",
      "...layers\\gaussian_noise\n",
      "......vars\n",
      "...layers\\input_layer\n",
      "......vars\n",
      "...layers\\input_layer_1\n",
      "......vars\n",
      "...layers\\lstm\n",
      "......vars\n",
      "...layers\\lstm\\cell\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...metrics\\mean\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........2\n",
      ".........3\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-09-06 20:48:07         4645\n",
      "metadata.json                                  2023-09-06 20:48:07           64\n",
      "variables.h5                                   2023-09-06 20:48:07       924280\n"
     ]
    }
   ],
   "source": [
    "# save training results\n",
    "with open(results_path + '{}_X.pickle'.format(model_name), 'wb') as fo:\n",
    "    pickle.dump(trainHist, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "be3a5a2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\ml_projects\\rise-gh.git\\rise-gh\\model\\saved\\gh_lstm_w128_v3_X\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\ml_projects\\rise-gh.git\\rise-gh\\model\\saved\\gh_lstm_w128_v3_X\\assets\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "model.save(save_path + model_name + '_X')\n",
    "#encoded.save(save_path + model_name + '_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0784401c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
