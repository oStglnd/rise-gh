{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c35d1cc",
   "metadata": {},
   "source": [
    "# CNN/GNN/LSTM/Transformer Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195db0c9",
   "metadata": {},
   "source": [
    "## Import dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77453a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dee1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant paths\n",
    "home_path = os.path.dirname(os.getcwd())\n",
    "data_path = home_path + '\\\\data\\\\'\n",
    "plot_path = home_path + '\\\\plotting\\\\plots\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747b251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get merged data\n",
    "data = pd.read_csv(\n",
    "    data_path + 'data_merged.csv',\n",
    "    header=[0, 1],\n",
    "    index_col=[0, 1, 2, 3]\n",
    ")\n",
    "\n",
    "# convert index.date col to datetime\n",
    "data.index = data.index.set_levels(\n",
    "    levels=pd.to_datetime(data.index.get_level_values(3).values),\n",
    "    level=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f46624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X vars\n",
    "x_vars = [\n",
    "    ('pressure', 'DC_GP101'),\n",
    "    ('pressure', 'SMHI'),\n",
    "    ('flow', 'TA01_GP101_default'),\n",
    "    ('flow', 'FF01_GP101_default'),\n",
    "    ('temperatures', 'DC_GT401_GM401'),\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('temperatures', 'DC_GT301_damped'),\n",
    "    ('temperatures', 'DC_GT301_outdoor'),\n",
    "    ('humidity', 'TA01_GT10X_GM10X'),\n",
    "    ('humidity', 'DC_GT401_GM401'),\n",
    "    ('humidity', 'SMHI'),\n",
    "    ('wind', 'SMHI_direction'),\n",
    "    ('wind', 'SMHI_speed'),\n",
    "    ('setpoints', 'DC_GP101_default'),\n",
    "    ('setpoints', 'TA01_GT10X_GM10X_default')\n",
    "]\n",
    "\n",
    "# filter columns to keep only x_vars\n",
    "data = data[x_vars].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae9eb6",
   "metadata": {},
   "source": [
    "## Describe Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be306b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of NAs\n",
    "print('Number of NAs\\n')\n",
    "print(data.isna().sum())\n",
    "print('\\n\\n')\n",
    "\n",
    "# check for NaNs\n",
    "nadat = data.droplevel(level=0, axis=1)[[\n",
    "    'TA01_GP101_default', \n",
    "    'FF01_GP101_default',\n",
    "    'DC_GP101_default',\n",
    "    'TA01_GT10X_GM10X_default'\n",
    "]]\n",
    "\n",
    "# delete Nan-data\n",
    "del nadat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2562a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['mins'] = data.index.get_level_values(3)\n",
    "data['mins'] = data.mins.apply(lambda d: d.strftime('%m-%d-%Y %H:%M'))\n",
    "\n",
    "data = data.groupby(['mins']).agg('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee866674",
   "metadata": {},
   "source": [
    "## Process / transform variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd77e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outdoors pressure in kPa-ish\n",
    "data[('pressure', 'SMHI')] = data.pressure.SMHI.values - data.pressure.SMHI.min()\n",
    "\n",
    "# remove erroneous setpoints data\n",
    "data[data.setpoints.TA01_GT10X_GM10X_default == 0.0].TA01_GT10X_GM10X_default = 20.0\n",
    "data[data.setpoints.DC_GP101_default == 0.0].DC_GP101_default = 4.0\n",
    "\n",
    "# Use temperature Diff. w.r.t. setpoint\n",
    "data[('temperatures', 'TA01_GT10X_GM10X_diff')] = \\\n",
    "    data[('temperatures', 'TA01_GT10X_GM10X')] - data[('setpoints', 'TA01_GT10X_GM10X_default')]\n",
    "\n",
    "# Use rolling avg. f. indoor humidity to filter out \n",
    "# effect of humidifier\n",
    "data[('humidity', 'TA01_GT10X_GM10X')] = data.humidity.TA01_GT10X_GM10X.rolling(\n",
    "    window=10,\n",
    "    center=True\n",
    ").mean().values\n",
    "\n",
    "# Use rolling avg. DC-GH humidity\n",
    "#data[('humidity', 'DC_GT401_GM401')] = data.humidity.DC_GT401_GM401.rolling(\n",
    "#    window=10,\n",
    "#    center=True,\n",
    "#).mean().values\n",
    "\n",
    "# remove NaNs from rolling avgs. (i.e. first 10 observations in data)\n",
    "data = data[data.humidity.TA01_GT10X_GM10X.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6899c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable f. time to account for time-specific effects (e.g. transpiration)\n",
    "\n",
    "#data[('time', 'minutes')] = data.index.get_level_values(3)\n",
    "data[('time', 'minutes')] = data.index.get_level_values(0)\n",
    "data[('time', 'min_sin')] = data.time.minutes.apply(\n",
    "    lambda d: abs(np.sin((d.hour * 60 + d.minute) * (2 * np.pi / (24 * 60 * 60))))\n",
    ")\n",
    "data[('time', 'min_cos')] = data.time.minutes.apply(\n",
    "    lambda d: abs(np.cos((d.hour * 60 + d.minute) * (2 * np.pi / (24 * 60 * 60))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2485723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable f. wind distribution\n",
    "wv = data.pop(('wind', 'SMHI_speed'))\n",
    "\n",
    "# Convert to radians.\n",
    "wd_rad = data.pop(('wind', 'SMHI_direction'))*np.pi / 180\n",
    "\n",
    "# Calculate the wind x and y components.\n",
    "data[('wind', 'Wx')] = wv*np.cos(wd_rad)\n",
    "data[('wind', 'Wy')] = wv*np.sin(wd_rad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcd953e",
   "metadata": {},
   "source": [
    "### Plot pairwise correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9109f1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(corr, mask=mask, vmin=-1.0, vmax=1.0, annot=True)\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Pairwise correlation plot')\n",
    "plt.show()\n",
    "\n",
    "# delete correlation data\n",
    "del corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70925b7a",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ed7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET N_STEPS\n",
    "n_steps = 30\n",
    "t_steps = 10\n",
    "\n",
    "### MASK f. TRAIN/TEST SPLIT\n",
    "mask = data.index.get_level_values(0) == 2\n",
    "\n",
    "### GET MODEL VARS\n",
    "model_vars = [\n",
    "    ('pressure', 'DC_GP101'),\n",
    "    ('pressure', 'SMHI'),\n",
    "    ('flow', 'TA01_GP101_default'),\n",
    "    ('flow', 'FF01_GP101_default'),\n",
    "    ('temperatures', 'DC_GT401_GM401'),\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('temperatures', 'TA01_GT10X_GM10X_diff'),\n",
    "    ('temperatures', 'DC_GT301_damped'),\n",
    "    ('temperatures', 'DC_GT301_outdoor'),\n",
    "    ('humidity', 'TA01_GT10X_GM10X'),\n",
    "    ('humidity', 'DC_GT401_GM401'),\n",
    "    ('humidity', 'SMHI'),\n",
    "    ('wind', 'Wx'),\n",
    "    ('wind', 'Wy'),\n",
    "    #('setpoints', 'DC_GP101_default'),\n",
    "    #('setpoints', 'TA01_GT10X_GM10X_default'),\n",
    "    #('time', 'min_sin'),\n",
    "    #('time', 'min_cos')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train & test data\n",
    "data_train = data[~mask][model_vars].copy()\n",
    "data_test  = data[mask][model_vars].copy()\n",
    "\n",
    "# delete original dataset & filtering mask\n",
    "del data, mask\n",
    "\n",
    "# get mean & std f. whitening\n",
    "mean = np.mean(\n",
    "    data_train.values, \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "std  = np.std(\n",
    "    data_train.values, \n",
    "    axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da753c48",
   "metadata": {},
   "source": [
    "### Erroneous seqeunces\n",
    "\n",
    "Since we have sequential data and missing/removed days, we will have some outlier series for which there is an \"erroneous\" break. Since e.g. December $16$ is removed from data, there will be sequences for which data starts December $15$ and continues/end in December $17$, and so on. We want to deal with these cases by removing the first number of sequences in a day corresponding to the number of **n_steps**, for when that day is preceded by a *deleted day*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff921a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_flagger(data, n_steps):\n",
    "    # get flagged dates\n",
    "    data['day'] = data.index.get_level_values(1).values\n",
    "    data['date_flag'] = data.day - data.day.shift(periods=n_steps) > 1\n",
    "\n",
    "    # get positions in data, w.r.t. n_step removed observations at start\n",
    "    flagged_idx = np.where(data.date_flag.values == 1)\n",
    "    flagged_idx = flagged_idx[0] - n_steps\n",
    "    \n",
    "    del data['day'], data['date_flag']\n",
    "    \n",
    "    return flagged_idx\n",
    "\n",
    "def data_split(data, n_steps, t_steps, mean, std):\n",
    "    \"\"\"\n",
    "    Split and whiten data. Using training mean and s.d. also for test data.\n",
    "    \"\"\"\n",
    "    # get flagged dates, accounting for clipped \n",
    "    flags = date_flagger(\n",
    "        data, \n",
    "        n_steps\n",
    "    )\n",
    "    \n",
    "    # get numeric data\n",
    "    dvals = (data.values - mean) / std\n",
    "    n = len(data)\n",
    "    \n",
    "    # delete dataset\n",
    "    del data\n",
    "    \n",
    "    # get all n_step-sequences from data series\n",
    "    sequences = np.stack([\n",
    "        dvals[i:i+n_steps, :] for i in range(n - n_steps)\n",
    "    ]) \n",
    "    \n",
    "    # get mask f. flags\n",
    "    mask = [idx not in flags for idx in range(len(sequences))]\n",
    "    sequences = sequences[mask]\n",
    "    \n",
    "    # get targets, delete mask\n",
    "    targets = dvals[n_steps:][mask]\n",
    "    del mask\n",
    "    \n",
    "    # clip t_steps from sequences and targets\n",
    "    sequences = sequences[:-t_steps]\n",
    "    targets = targets[t_steps:]\n",
    "    \n",
    "    # return sequences w. channel\n",
    "    #return sequences[..., np.newaxis]\n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002eaf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequences f. training and test data\n",
    "X_train, Y_train = data_split(data_train, n_steps=n_steps, t_steps=t_steps, mean=mean, std=std)\n",
    "X_test, Y_test = data_split(data_test, n_steps=n_steps, t_steps=t_steps, mean=mean, std=std)\n",
    "\n",
    "# filter Y-data\n",
    "####Y_train = Y_train[mask]\n",
    "Y_train = Y_train[:, 6]\n",
    "Y_test = Y_test[:, 6]\n",
    "\n",
    "# delete data sets\n",
    "del data_train, data_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902451d",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90e0424",
   "metadata": {},
   "outputs": [],
   "source": [
    "layerList = [\n",
    "    layers.Input(shape=(\n",
    "        X_train.shape[1],\n",
    "        X_train.shape[2]\n",
    "    )),\n",
    "    layers.Conv1D(\n",
    "        filters=128,\n",
    "        kernel_size=7, \n",
    "        padding=\"same\", \n",
    "        strides=2, \n",
    "        activation=\"relu\"\n",
    "    ),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    layers.Conv1D(\n",
    "        filters=64,\n",
    "        kernel_size=7, \n",
    "        padding=\"same\", \n",
    "        strides=2, \n",
    "        activation=\"relu\"\n",
    "    ),\n",
    "    layers.Dropout(rate=0.5),\n",
    "    #layers.Flatten(),\n",
    "    layers.LSTM(\n",
    "        units=32,\n",
    "        activation='tanh',\n",
    "        recurrent_activation='sigmoid',\n",
    "    ),\n",
    "    #layers.Dense(128, activation='relu'),\n",
    "    #layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(1, activation=None)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ba22a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(layers=layerList, name='model')\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e592436",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainHist = model.fit(\n",
    "    X_train,\n",
    "    Y_train,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.1,\n",
    "    #callbacks=[\n",
    "    #    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "    #],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b54187",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec86de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
