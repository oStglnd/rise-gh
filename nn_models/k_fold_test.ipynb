{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c35d1cc",
   "metadata": {},
   "source": [
    "# Greenhouse Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195db0c9",
   "metadata": {},
   "source": [
    "## Import dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77453a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0186ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prop. funcs and models\n",
    "\n",
    "from data_funcs import k_fold_data_validation\n",
    "from train_funcs import train_network, test_autoreg\n",
    "from networks import feedForwardNeuralNetwork, recurrentNeuralNetwork\n",
    "from opts import AdaGrad, RMSProp, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a564526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external models\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ace44",
   "metadata": {},
   "source": [
    "## Create data for specific fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594fb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant paths\n",
    "home_path = os.path.dirname(os.getcwd())\n",
    "data_path = home_path + '\\\\data\\\\'\n",
    "results_path = home_path + '\\\\nn_models\\\\results\\\\'\n",
    "\n",
    "# get merged data\n",
    "data = pd.read_csv(\n",
    "    data_path + 'data_processed.csv',\n",
    "    header=[0, 1],\n",
    "    index_col=[0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aa55a697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define data specs\n",
    "k_frac = 0.05\n",
    "m = 5\n",
    "t_steps = 1\n",
    "n_steps = 6\n",
    "\n",
    "# Define cols to use\n",
    "cols = [\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('temperatures', 'TA01_GT401_GM401'),\n",
    "    ('temperatures', 'DC_GT301_damped'),\n",
    "    ('sun', 'gsi'),\n",
    "    ('power', 'phase'),\n",
    "#     ('time', 'minofday'),\n",
    "#     ('temperatures', 'TA01_GT10X_GM10X_loss'),\n",
    "#     ('humidity', 'TA01_GT10X_GM10X_abs'),\n",
    "#     ('humidity', 'TA01_GT401_GM401_abs'),\n",
    "#     ('humidity', 'outdoor_abs'),\n",
    "    ('temperatures', 'TA01_GT401_GM401_scaled'),\n",
    "    ('temperatures', 'DC_GT301_damped_scaled'),\n",
    "#     ('state', 'TA01_output'),\n",
    "#     ('state', 'TA02_output')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187819d",
   "metadata": {},
   "source": [
    "## K-fold CV - Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccc61265",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7778e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams\n",
    "k1 = 6\n",
    "k2 = 1\n",
    "lambd = 0.01\n",
    "sigma = 1.0\n",
    "seed = 1\n",
    "\n",
    "# more params\n",
    "n_epochs = 10\n",
    "n_batch = 128\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40772677",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_idx in range(20):\n",
    "    train_tup, test_tup, val_tup, col_params = k_fold_data_validation(\n",
    "        data=data.copy(), \n",
    "        k_idx=k_idx, \n",
    "        k_frac=k_frac, \n",
    "        m=m, \n",
    "        cols=cols, \n",
    "        t_steps=t_steps, \n",
    "        n_steps=n_steps,\n",
    "        setpoint=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # extract tuples\n",
    "    (sequences_train, targets_train, temps_train, temps_t_train, _) = train_tup\n",
    "    (sequences_test, targets_test, temps_test, temps_t_test, sequences_masked, _) = test_tup\n",
    "    (sequences_val, targets_val, temps_val, temps_t_val, _) = val_tup\n",
    "    \n",
    "#     ### ----- FEED-FORWARD NN -----\n",
    "#     # define network\n",
    "#     model_name = 'FFNN_v{}_k{}'.format(version, k_idx)\n",
    "#     units=[32, 32, 32]\n",
    "#     model = feedForwardNeuralNetwork(\n",
    "#         k1=k1,\n",
    "#         k2=k2,\n",
    "#         m=units,\n",
    "#         seed=seed\n",
    "#     )\n",
    "    \n",
    "#     # define optimizer\n",
    "#     adam = Adam(\n",
    "#         beta1=0.9,\n",
    "#         beta2=0.999,\n",
    "#         eps=1e-8,\n",
    "#         weights=model.weights,\n",
    "#     )\n",
    "    \n",
    "#     # set optimizer\n",
    "#     model.optimizer = adam\n",
    "    \n",
    "#     # train model\n",
    "#     results = train_network(\n",
    "#         model=model,\n",
    "#         train_data=(sequences_train[:, -2, :], temps_t_train, targets_train),\n",
    "#         val_data=(sequences_val[:, -2, :], temps_t_val, targets_val),\n",
    "#         seed=seed,\n",
    "#         n_epochs=n_epochs,\n",
    "#         n_batch=n_batch,\n",
    "#         lambd=lambd,\n",
    "#         sigma=sigma,\n",
    "#         lr=lr,\n",
    "#         optimizer='adam'\n",
    "#     )\n",
    "    \n",
    "#     # get autoregressive test predictions\n",
    "#     test_preds, test_encodings = test_autoreg(\n",
    "#         model,\n",
    "#         sequences_masked[:, -2, :],\n",
    "#         temps_t_test,\n",
    "#         targets_test,\n",
    "#         t_steps\n",
    "#     )\n",
    "    \n",
    "#     results['test_preds'] = test_preds\n",
    "#     results['test_encodings'] = test_encodings\n",
    "    \n",
    "#     # save results\n",
    "#     save_path = results_path + 'ffnn\\\\' + model_name + '.pickle'\n",
    "#     with open(save_path, 'wb') as fo:\n",
    "#         pickle.dump(results, fo)\n",
    "    \n",
    "#     # clear output\n",
    "#     clear_output()\n",
    "    \n",
    "    \n",
    "    ### ----- RECURRENT NN -----\n",
    "    # define network\n",
    "    model_name = 'RNN_v{}_k{}'.format(version, k_idx)\n",
    "    units = 32\n",
    "    model = recurrentNeuralNetwork(\n",
    "        k1=k1,\n",
    "        k2=k2,\n",
    "        m=units,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        eps=1e-8,\n",
    "        weights=model.weights,\n",
    "    )\n",
    "    \n",
    "    # set optimizer\n",
    "    model.optimizer = adam\n",
    "    \n",
    "    # train model\n",
    "    results = train_network(\n",
    "        model=model,\n",
    "        train_data=(sequences_train, temps_t_train, targets_train),\n",
    "        val_data=(sequences_val, temps_t_val, targets_val),\n",
    "        seed=seed,\n",
    "        n_epochs=n_epochs,\n",
    "        n_batch=n_batch,\n",
    "        lambd=lambd,\n",
    "        sigma=sigma,\n",
    "        lr=lr,\n",
    "        optimizer='adam'\n",
    "    )\n",
    " \n",
    "    # get autoregressive test predictions\n",
    "    test_preds, test_encodings = test_autoreg(\n",
    "        model,\n",
    "        sequences_masked,\n",
    "        temps_t_test,\n",
    "        targets_test,\n",
    "        t_steps\n",
    "    )\n",
    "    \n",
    "    results['test_preds'] = test_preds\n",
    "    results['test_encodings'] = test_encodings\n",
    "\n",
    "    # save results\n",
    "    save_path = results_path + 'rnn\\\\' + model_name + '.pickle'\n",
    "    with open(save_path, 'wb') as fo:\n",
    "        pickle.dump(results, fo)\n",
    "\n",
    "    # clear output\n",
    "    clear_output()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e710b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # units=[32, 32, 32]\n",
    "# # model = feedForwardNeuralNetwork(\n",
    "# #     k1=k1,\n",
    "# #     k2=k2,\n",
    "# #     m=units,\n",
    "# #     seed=seed\n",
    "# # )\n",
    "\n",
    "# units = 32\n",
    "# model = recurrentNeuralNetwork(\n",
    "#     k1=k1,\n",
    "#     k2=k2,\n",
    "#     m=units,\n",
    "#     seed=seed\n",
    "# )\n",
    "\n",
    "# weight_count = 0\n",
    "# for weight in model.weights.values():\n",
    "#     weight_count += weight.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af03e42",
   "metadata": {},
   "source": [
    "## K-fold CV - other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fad0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k_idx in range(20):\n",
    "#     train_tup, test_tup, val_tup, col_params = k_fold_data_validation(\n",
    "#         data=data.copy(), \n",
    "#         k_idx=k_idx, \n",
    "#         k_frac=k_frac, \n",
    "#         m=m, \n",
    "#         cols=cols, \n",
    "#         t_steps=t_steps, \n",
    "#         n_steps=n_steps,\n",
    "#         setpoint=True,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "    \n",
    "#     # extract tuples\n",
    "#     (sequences_train, targets_train, temps_train, temps_t_train, _) = train_tup\n",
    "#     (sequences_test, targets_test, temps_test, temps_t_test, sequences_masked, _) = test_tup\n",
    "#     (sequences_val, targets_val, temps_val, temps_t_val, _) = val_tup\n",
    "\n",
    "#     ### TRAIN ARIMA\n",
    "#     clear_output()\n",
    "#     print('TRAINING ARIMA, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "#     # get model name\n",
    "#     model_name = 'arima_v{}_k{}'.format(version, k_idx)\n",
    "    \n",
    "#     # get endogenous and exogenous regressors\n",
    "#     endog_train = temps_t_train.tolist()\n",
    "#     exog_train = [np.array(seq) for seq in sequences_train[:, -2, :].tolist()]\n",
    "# #     exog_train = [np.array(seq).mean(axis=0) for seq in sequences_train[:, -t_steps:, :].tolist()]\n",
    "\n",
    "#     endog_test = temps_t_test.tolist()\n",
    "#     exog_test = [np.array(seq) for seq in sequences_masked[:, -2, :].tolist()]\n",
    "# #     exog_test = [np.array(seq).mean(axis=0) for seq in sequences_test[:, -t_steps:, :].tolist()]\n",
    "    \n",
    "#     # estimate model\n",
    "#     start = time.time()\n",
    "#     arima_temp = ARIMA(endog=endog_train, exog=exog_train, order=(n_steps, 1, 0)).fit()\n",
    "#     train_time = time.time() - start\n",
    "    \n",
    "#     # get predictions (t steps)\n",
    "#     print('PREDICTING w. ARIMA, k_idx: {}'.format(k_idx))\n",
    "#     arima_preds = []\n",
    "#     for exog in exog_test:\n",
    "#         pred = arima_temp.forecast(steps=1, exog=exog)\n",
    "#         arima_preds.append(pred)\n",
    "\n",
    "#     save_path = results_path + 'arima\\\\' + model_name\n",
    "#     #arima_temp.save(save_path + '_model')\n",
    "    \n",
    "#     results = {\n",
    "#         'test_preds':arima_preds,\n",
    "#         'train_time':train_time\n",
    "#     }\n",
    "#     with open(save_path + '.pickle', 'wb') as fo:\n",
    "#         pickle.dump(results, fo)\n",
    "        \n",
    "#     ### TRAIN GBDT\n",
    "#     clear_output()\n",
    "#     print('TRAINING GBDT, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "#     # get model name\n",
    "#     model_name = 'gbdt_v{}_k{}'.format(version, k_idx)\n",
    "#     gbdt = GradientBoostingRegressor(\n",
    "#         loss='squared_error',\n",
    "#         learning_rate=0.001,\n",
    "#         n_estimators=2000,\n",
    "#         max_depth=10,\n",
    "#         max_leaf_nodes=None,\n",
    "#         #subsample=0.7,\n",
    "#         verbose=1,\n",
    "# #         n_iter_no_change=10,\n",
    "# #         tol=1e-4,\n",
    "# #         validation_fraction=0.1\n",
    "#     )\n",
    "    \n",
    "#     # train GBDT\n",
    "#     start = time.time()\n",
    "#     X_train = np.hstack((np.stack(exog_train), temps_t_train))\n",
    "#     gbdt.fit(X_train, targets_train.flatten())\n",
    "#     train_time = time.time() - start\n",
    "    \n",
    "#     # create queue for preds\n",
    "#     pred_queue = deque(maxlen=t_steps)\n",
    "#     for temp in temps_t_test[:t_steps]:\n",
    "#         pred_queue.append(temp)\n",
    "    \n",
    "#     # iterate over test seqs and get preds\n",
    "#     gbdt_preds = []\n",
    "#     X_test = [np.array(seq) for seq in sequences_masked[:, -2, :].tolist()]\n",
    "#     X_test = np.hstack((np.stack(X_test), temps_t_test))\n",
    "\n",
    "#     for x in X_test:\n",
    "#         temp = np.array([pred_queue.popleft()])\n",
    "#         x[-1] = temp[0][0]\n",
    "#         temp_pred = gbdt.predict(x[np.newaxis, :])\n",
    "#         pred_queue.append(temp_pred)\n",
    "#         gbdt_preds.append(temp_pred)\n",
    "        \n",
    "#     # save model\n",
    "#     save_path = results_path + 'gbdt\\\\' + model_name\n",
    "# #     with open(save_path + '_model.pickle'.format(gbdt_name), 'wb') as fo:\n",
    "# #         pickle.dump(gbdt, fo)\n",
    "        \n",
    "#     # save model results\n",
    "#     results = {\n",
    "#         'test_preds':gbdt_preds,\n",
    "#         'train_loss':gbdt.train_score_,\n",
    "#         'train_time':train_time,\n",
    "#         'model_params':gbdt.get_params(),\n",
    "#     }\n",
    "#     with open(save_path + '.pickle', 'wb') as fo:\n",
    "#         pickle.dump(results, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b92882d",
   "metadata": {},
   "source": [
    "# K-fold CV - GRU and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "99a1bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "843a9415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(gru_or_lstm, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = layers.Input(shape=(sequences_train.shape[1], sequences_train.shape[2]))\n",
    "    \n",
    "    if gru_or_lstm.lower() == 'gru':\n",
    "        _, encoder = layers.GRU(\n",
    "            units=32, \n",
    "            recurrent_dropout=0.0, \n",
    "            return_sequences=False, \n",
    "            return_state=True, \n",
    "            activity_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "        )(inputs)\n",
    "    else:\n",
    "        _, _, encoder = layers.LSTM(\n",
    "            units=32, \n",
    "            recurrent_dropout=0.0, \n",
    "            return_sequences=False, \n",
    "            return_state=True, \n",
    "            activity_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "        )(inputs)\n",
    "        \n",
    "    temp_input = layers.Input(shape=(1,))\n",
    "    temp = layers.GaussianNoise(stddev=1.0)(temp_input)\n",
    "    temp = layers.BatchNormalization()(temp)\n",
    "\n",
    "    output = layers.Concatenate()([encoder, temp])\n",
    "    output = layers.Dense(units=1, activation=None, use_bias=False, activity_regularizer=tf.keras.regularizers.L2(0.01))(output)\n",
    "\n",
    "    model = Model([inputs, temp_input], output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c87e79d8",
   "metadata": {},
   "source": [
    "version = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ef7dfce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING LSTM, k_idx: 3\n",
      "Epoch 1/30\n",
      "382/382 [==============================] - 5s 11ms/step - loss: 0.2213\n",
      "Epoch 2/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.1213\n",
      "Epoch 3/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.1024\n",
      "Epoch 4/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0898\n",
      "Epoch 5/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0824\n",
      "Epoch 6/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0767\n",
      "Epoch 7/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0729\n",
      "Epoch 8/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0696\n",
      "Epoch 9/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0671\n",
      "Epoch 10/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0655\n",
      "Epoch 11/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0634\n",
      "Epoch 12/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0617\n",
      "Epoch 13/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0600\n",
      "Epoch 14/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0594\n",
      "Epoch 15/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0582\n",
      "Epoch 16/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0572\n",
      "Epoch 17/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0563\n",
      "Epoch 18/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0553\n",
      "Epoch 19/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0545\n",
      "Epoch 20/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0539\n",
      "Epoch 21/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0534\n",
      "Epoch 22/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0527\n",
      "Epoch 23/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0522\n",
      "Epoch 24/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0518\n",
      "Epoch 25/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0515\n",
      "Epoch 26/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0510\n",
      "Epoch 27/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0506\n",
      "Epoch 28/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0502\n",
      "Epoch 29/30\n",
      "382/382 [==============================] - 4s 11ms/step - loss: 0.0498\n",
      "Epoch 30/30\n",
      "382/382 [==============================] - 4s 10ms/step - loss: 0.0495\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19760\\68893403.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msequences_masked\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mtemp_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[0mpred_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2315\u001b[0m                     )\n\u001b[0;32m   2316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m             data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   2318\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m                 \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1259\u001b[1;33m         self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1260\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2335\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformation\u001b[0m \u001b[0mapplied\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdescribed\u001b[0m \u001b[0mabove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2336\u001b[0m     \"\"\"\n\u001b[1;32m-> 2337\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2339\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_warning\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m   5589\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_structure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_spec\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5590\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5591\u001b[1;33m     variant_tensor = gen_dataset_ops.flat_map_dataset(\n\u001b[0m\u001b[0;32m   5592\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5593\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mflat_map_dataset\u001b[1;34m(input_dataset, other_arguments, f, output_types, output_shapes, metadata, name)\u001b[0m\n\u001b[0;32m   2373\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2374\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2375\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2376\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FlatMapDataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother_arguments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"f\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2377\u001b[0m         \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_shapes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k_idx in range(20):\n",
    "    train_tup, test_tup, val_tup, col_params = k_fold_data_validation(\n",
    "        data=data.copy(), \n",
    "        k_idx=k_idx, \n",
    "        k_frac=k_frac, \n",
    "        m=m, \n",
    "        cols=cols, \n",
    "        t_steps=t_steps, \n",
    "        n_steps=n_steps,\n",
    "        setpoint=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # extract tuples\n",
    "    (sequences_train, targets_train, temps_train, temps_t_train, _) = train_tup\n",
    "    (sequences_test, targets_test, temps_test, temps_t_test, sequences_masked, _) = test_tup\n",
    "    (sequences_val, targets_val, temps_val, temps_t_val, _) = val_tup\n",
    "\n",
    "    ### TRAIN GRU\n",
    "    clear_output()\n",
    "    print('TRAINING GRU, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "    # get model name\n",
    "    model_name = 'gru_v{}_k{}'.format(version, k_idx)\n",
    "    \n",
    "    # get model\n",
    "    gru_model = get_model('gru', lr=0.001)\n",
    "    \n",
    "    # train model\n",
    "    trainHist = gru_model.fit(\n",
    "        [sequences_train, temps_train],\n",
    "        targets_train,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_split=0.0,\n",
    "    )\n",
    "    \n",
    "    # get autoregressive preds and encodings\n",
    "    preds = []\n",
    "#     encodings = []\n",
    "    \n",
    "    pred_queue = deque(maxlen=t_steps)\n",
    "    for temp in temps_test[:t_steps]:\n",
    "        pred_queue.append(temp[np.newaxis, :])\n",
    "        preds.append(temp)\n",
    "\n",
    "    for seq in sequences_masked:\n",
    "        temp = pred_queue.popleft()\n",
    "        temp_pred = gru_model.predict([seq[np.newaxis, :], temp], verbose=0)\n",
    "        pred_queue.append(temp_pred)\n",
    "        preds.append(temp_pred[0])\n",
    "#         encodings.append(encoded[0])\n",
    "\n",
    "    results = {}\n",
    "    results['test_preds'] = preds\n",
    "    results['test_encodings'] = encodings\n",
    "    # save results\n",
    "    save_path = results_path + 'gru\\\\' + model_name + '.pickle'\n",
    "    with open(save_path, 'wb') as fo:\n",
    "        pickle.dump(results, fo)\n",
    "\n",
    "    # clear output\n",
    "    clear_output()\n",
    "        \n",
    "    ### TRAIN LSTM\n",
    "    clear_output()\n",
    "    print('TRAINING LSTM, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "    # get model name\n",
    "    model_name = 'lstm_v{}_k{}'.format(version, k_idx)\n",
    "    \n",
    "    # get model\n",
    "    lstm_model = get_model('lstm', lr=0.001)\n",
    "    \n",
    "    # train model\n",
    "    trainHist = lstm_model.fit(\n",
    "        [sequences_train, temps_train],\n",
    "        targets_train,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        validation_split=0.0,\n",
    "    )\n",
    "    \n",
    "    # get autoregressive preds and encodings\n",
    "    preds = []\n",
    "#     encodings = []\n",
    "\n",
    "    pred_queue = deque(maxlen=t_steps)\n",
    "    for temp in temps_test[:t_steps]:\n",
    "        pred_queue.append(temp[np.newaxis, :])\n",
    "        preds.append(temp)\n",
    "\n",
    "    for seq in sequences_masked:\n",
    "        temp = pred_queue.popleft()\n",
    "        temp_pred = lstm_model.predict([seq[np.newaxis, :], temp], verbose=0)\n",
    "        pred_queue.append(temp_pred)\n",
    "        preds.append(temp_pred[0])\n",
    "#         encodings.append(encoded[0])\n",
    "        \n",
    "    results = {}\n",
    "    results['test_preds'] = preds\n",
    "    results['test_encodings'] = encodings\n",
    "    # save results\n",
    "    save_path = results_path + 'lstm\\\\' + model_name + '.pickle'\n",
    "    with open(save_path, 'wb') as fo:\n",
    "        pickle.dump(results, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b15b10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
