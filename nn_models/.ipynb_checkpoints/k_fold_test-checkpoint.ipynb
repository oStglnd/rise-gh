{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c35d1cc",
   "metadata": {},
   "source": [
    "# Greenhouse Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195db0c9",
   "metadata": {},
   "source": [
    "## Import dependencies and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77453a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import json\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0186ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import prop. funcs and models\n",
    "\n",
    "from data_funcs import k_fold_data_validation\n",
    "from train_funcs import train_network, test_autoreg\n",
    "from networks import feedForwardNeuralNetwork, recurrentNeuralNetwork\n",
    "from opts import AdaGrad, RMSProp, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a564526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external models\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "# from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ace44",
   "metadata": {},
   "source": [
    "## Create data for specific fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "594fb174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define relevant paths\n",
    "home_path = os.path.dirname(os.getcwd())\n",
    "data_path = home_path + '\\\\data\\\\'\n",
    "results_path = home_path + '\\\\nn_models\\\\results\\\\'\n",
    "\n",
    "# get merged data\n",
    "data = pd.read_csv(\n",
    "    data_path + 'data_processed.csv',\n",
    "    header=[0, 1],\n",
    "    index_col=[0, 1, 2, 3, 4]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa55a697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define data specs\n",
    "k_frac = 0.05\n",
    "m = 5\n",
    "t_steps = 1\n",
    "n_steps = 3\n",
    "\n",
    "# Define cols to use\n",
    "cols = [\n",
    "    ('temperatures', 'TA01_GT10X_GM10X'),\n",
    "    ('temperatures', 'TA01_GT401_GM401'),\n",
    "    ('temperatures', 'DC_GT301_damped'),\n",
    "    ('sun', 'gsi'),\n",
    "    ('power', 'phase'),\n",
    "#     ('time', 'minofday'),\n",
    "#     ('temperatures', 'TA01_GT10X_GM10X_loss'),\n",
    "#     ('humidity', 'TA01_GT10X_GM10X_abs'),\n",
    "#     ('humidity', 'TA01_GT401_GM401_abs'),\n",
    "#     ('humidity', 'outdoor_abs'),\n",
    "    ('temperatures', 'TA01_GT401_GM401_scaled'),\n",
    "    ('temperatures', 'DC_GT301_damped_scaled'),\n",
    "#     ('state', 'TA01_output'),\n",
    "#     ('state', 'TA02_output')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187819d",
   "metadata": {},
   "source": [
    "## K-fold CV - Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccc61265",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7778e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training hyperparams\n",
    "k1 = 6\n",
    "k2 = 1\n",
    "lambd = 0.01\n",
    "sigma = 1.0\n",
    "seed = 1\n",
    "\n",
    "# more params\n",
    "n_epochs = 10\n",
    "n_batch = 128\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "40772677",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k_idx in range(20):\n",
    "    train_tup, test_tup, val_tup, col_params = k_fold_data_validation(\n",
    "        data=data.copy(), \n",
    "        k_idx=k_idx, \n",
    "        k_frac=k_frac, \n",
    "        m=m, \n",
    "        cols=cols, \n",
    "        t_steps=t_steps, \n",
    "        n_steps=n_steps,\n",
    "        setpoint=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # extract tuples\n",
    "    (sequences_train, targets_train, temps_train, temps_t_train, _) = train_tup\n",
    "    (sequences_test, targets_test, temps_test, temps_t_test, sequences_masked, _) = test_tup\n",
    "    (sequences_val, targets_val, temps_val, temps_t_val, _) = val_tup\n",
    "    \n",
    "#     ### ----- FEED-FORWARD NN -----\n",
    "#     # define network\n",
    "#     model_name = 'FFNN_v{}_k{}'.format(version, k_idx)\n",
    "#     units=[32, 32, 32]\n",
    "#     model = feedForwardNeuralNetwork(\n",
    "#         k1=k1,\n",
    "#         k2=k2,\n",
    "#         m=units,\n",
    "#         seed=seed\n",
    "#     )\n",
    "    \n",
    "#     # define optimizer\n",
    "#     adam = Adam(\n",
    "#         beta1=0.9,\n",
    "#         beta2=0.999,\n",
    "#         eps=1e-8,\n",
    "#         weights=model.weights,\n",
    "#     )\n",
    "    \n",
    "#     # set optimizer\n",
    "#     model.optimizer = adam\n",
    "    \n",
    "#     # train model\n",
    "#     results = train_network(\n",
    "#         model=model,\n",
    "#         train_data=(sequences_train[:, -2, :], temps_t_train, targets_train),\n",
    "#         val_data=(sequences_val[:, -2, :], temps_t_val, targets_val),\n",
    "#         seed=seed,\n",
    "#         n_epochs=n_epochs,\n",
    "#         n_batch=n_batch,\n",
    "#         lambd=lambd,\n",
    "#         sigma=sigma,\n",
    "#         lr=lr,\n",
    "#         optimizer='adam'\n",
    "#     )\n",
    "    \n",
    "#     # get autoregressive test predictions\n",
    "#     test_preds, test_encodings = test_autoreg(\n",
    "#         model,\n",
    "#         sequences_masked[:, -2, :],\n",
    "#         temps_t_test,\n",
    "#         targets_test,\n",
    "#         t_steps\n",
    "#     )\n",
    "    \n",
    "#     results['test_preds'] = test_preds\n",
    "#     results['test_encodings'] = test_encodings\n",
    "    \n",
    "#     # save results\n",
    "#     save_path = results_path + 'ffnn\\\\' + model_name + '.pickle'\n",
    "#     with open(save_path, 'wb') as fo:\n",
    "#         pickle.dump(results, fo)\n",
    "    \n",
    "#     # clear output\n",
    "#     clear_output()\n",
    "    \n",
    "    \n",
    "    ### ----- RECURRENT NN -----\n",
    "    # define network\n",
    "    model_name = 'RNN_v{}_k{}'.format(version, k_idx)\n",
    "    units = 32\n",
    "    model = recurrentNeuralNetwork(\n",
    "        k1=k1,\n",
    "        k2=k2,\n",
    "        m=units,\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # define optimizer\n",
    "    adam = Adam(\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        eps=1e-8,\n",
    "        weights=model.weights,\n",
    "    )\n",
    "    \n",
    "    # set optimizer\n",
    "    model.optimizer = adam\n",
    "    \n",
    "    # train model\n",
    "    results = train_network(\n",
    "        model=model,\n",
    "        train_data=(sequences_train, temps_t_train, targets_train),\n",
    "        val_data=(sequences_val, temps_t_val, targets_val),\n",
    "        seed=seed,\n",
    "        n_epochs=n_epochs,\n",
    "        n_batch=n_batch,\n",
    "        lambd=lambd,\n",
    "        sigma=sigma,\n",
    "        lr=lr,\n",
    "        optimizer='adam'\n",
    "    )\n",
    " \n",
    "    # get autoregressive test predictions\n",
    "    test_preds, test_encodings = test_autoreg(\n",
    "        model,\n",
    "        sequences_masked,\n",
    "        temps_t_test,\n",
    "        targets_test,\n",
    "        t_steps\n",
    "    )\n",
    "    \n",
    "    results['test_preds'] = test_preds\n",
    "    results['test_encodings'] = test_encodings\n",
    "\n",
    "    # save results\n",
    "    save_path = results_path + 'rnn\\\\' + model_name + '.pickle'\n",
    "    with open(save_path, 'wb') as fo:\n",
    "        pickle.dump(results, fo)\n",
    "\n",
    "    # clear output\n",
    "    clear_output()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e710b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # units=[32, 32, 32]\n",
    "# # model = feedForwardNeuralNetwork(\n",
    "# #     k1=k1,\n",
    "# #     k2=k2,\n",
    "# #     m=units,\n",
    "# #     seed=seed\n",
    "# # )\n",
    "\n",
    "# units = 32\n",
    "# model = recurrentNeuralNetwork(\n",
    "#     k1=k1,\n",
    "#     k2=k2,\n",
    "#     m=units,\n",
    "#     seed=seed\n",
    "# )\n",
    "\n",
    "# weight_count = 0\n",
    "# for weight in model.weights.values():\n",
    "#     weight_count += weight.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af03e42",
   "metadata": {},
   "source": [
    "## K-fold CV - other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fad0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k_idx in range(20):\n",
    "#     train_tup, test_tup, val_tup, col_params = k_fold_data_validation(\n",
    "#         data=data.copy(), \n",
    "#         k_idx=k_idx, \n",
    "#         k_frac=k_frac, \n",
    "#         m=m, \n",
    "#         cols=cols, \n",
    "#         t_steps=t_steps, \n",
    "#         n_steps=n_steps,\n",
    "#         setpoint=True,\n",
    "#         shuffle=False\n",
    "#     )\n",
    "    \n",
    "#     # extract tuples\n",
    "#     (sequences_train, targets_train, temps_train, temps_t_train, _) = train_tup\n",
    "#     (sequences_test, targets_test, temps_test, temps_t_test, sequences_masked, _) = test_tup\n",
    "#     (sequences_val, targets_val, temps_val, temps_t_val, _) = val_tup\n",
    "\n",
    "#     ### TRAIN ARIMA\n",
    "#     clear_output()\n",
    "#     print('TRAINING ARIMA, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "#     # get model name\n",
    "#     model_name = 'arima_v{}_k{}'.format(version, k_idx)\n",
    "    \n",
    "#     # get endogenous and exogenous regressors\n",
    "#     endog_train = temps_t_train.tolist()\n",
    "#     exog_train = [np.array(seq) for seq in sequences_train[:, -2, :].tolist()]\n",
    "# #     exog_train = [np.array(seq).mean(axis=0) for seq in sequences_train[:, -t_steps:, :].tolist()]\n",
    "\n",
    "#     endog_test = temps_t_test.tolist()\n",
    "#     exog_test = [np.array(seq) for seq in sequences_masked[:, -2, :].tolist()]\n",
    "# #     exog_test = [np.array(seq).mean(axis=0) for seq in sequences_test[:, -t_steps:, :].tolist()]\n",
    "    \n",
    "#     # estimate model\n",
    "#     start = time.time()\n",
    "#     arima_temp = ARIMA(endog=endog_train, exog=exog_train, order=(n_steps, 1, 0)).fit()\n",
    "#     train_time = time.time() - start\n",
    "    \n",
    "#     # get predictions (t steps)\n",
    "#     print('PREDICTING w. ARIMA, k_idx: {}'.format(k_idx))\n",
    "#     arima_preds = []\n",
    "#     for exog in exog_test:\n",
    "#         pred = arima_temp.forecast(steps=1, exog=exog)\n",
    "#         arima_preds.append(pred)\n",
    "\n",
    "#     save_path = results_path + 'arima\\\\' + model_name\n",
    "#     #arima_temp.save(save_path + '_model')\n",
    "    \n",
    "#     results = {\n",
    "#         'test_preds':arima_preds,\n",
    "#         'train_time':train_time\n",
    "#     }\n",
    "#     with open(save_path + '.pickle', 'wb') as fo:\n",
    "#         pickle.dump(results, fo)\n",
    "        \n",
    "#     ### TRAIN GBDT\n",
    "#     clear_output()\n",
    "#     print('TRAINING GBDT, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "#     # get model name\n",
    "#     model_name = 'gbdt_v{}_k{}'.format(version, k_idx)\n",
    "#     gbdt = GradientBoostingRegressor(\n",
    "#         loss='squared_error',\n",
    "#         learning_rate=0.001,\n",
    "#         n_estimators=2000,\n",
    "#         max_depth=10,\n",
    "#         max_leaf_nodes=None,\n",
    "#         #subsample=0.7,\n",
    "#         verbose=1,\n",
    "# #         n_iter_no_change=10,\n",
    "# #         tol=1e-4,\n",
    "# #         validation_fraction=0.1\n",
    "#     )\n",
    "    \n",
    "#     # train GBDT\n",
    "#     start = time.time()\n",
    "#     X_train = np.hstack((np.stack(exog_train), temps_t_train))\n",
    "#     gbdt.fit(X_train, targets_train.flatten())\n",
    "#     train_time = time.time() - start\n",
    "    \n",
    "#     # create queue for preds\n",
    "#     pred_queue = deque(maxlen=t_steps)\n",
    "#     for temp in temps_t_test[:t_steps]:\n",
    "#         pred_queue.append(temp)\n",
    "    \n",
    "#     # iterate over test seqs and get preds\n",
    "#     gbdt_preds = []\n",
    "#     X_test = [np.array(seq) for seq in sequences_masked[:, -2, :].tolist()]\n",
    "#     X_test = np.hstack((np.stack(X_test), temps_t_test))\n",
    "\n",
    "#     for x in X_test:\n",
    "#         temp = np.array([pred_queue.popleft()])\n",
    "#         x[-1] = temp[0][0]\n",
    "#         temp_pred = gbdt.predict(x[np.newaxis, :])\n",
    "#         pred_queue.append(temp_pred)\n",
    "#         gbdt_preds.append(temp_pred)\n",
    "        \n",
    "#     # save model\n",
    "#     save_path = results_path + 'gbdt\\\\' + model_name\n",
    "# #     with open(save_path + '_model.pickle'.format(gbdt_name), 'wb') as fo:\n",
    "# #         pickle.dump(gbdt, fo)\n",
    "        \n",
    "#     # save model results\n",
    "#     results = {\n",
    "#         'test_preds':gbdt_preds,\n",
    "#         'train_loss':gbdt.train_score_,\n",
    "#         'train_time':train_time,\n",
    "#         'model_params':gbdt.get_params(),\n",
    "#     }\n",
    "#     with open(save_path + '.pickle', 'wb') as fo:\n",
    "#         pickle.dump(results, fo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64330a86",
   "metadata": {},
   "source": [
    "# K-fold CV - GRU and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7f1f674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2f38d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(gru_or_lstm, lr):\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    inputs = layers.Input(shape=(sequences_train.shape[1], sequences_train.shape[2]))\n",
    "    \n",
    "    if gru_or_lstm.lower() == 'gru':\n",
    "        _, encoder = layers.GRU(\n",
    "            units=32, \n",
    "            recurrent_dropout=0.0, \n",
    "            return_sequences=False, \n",
    "            return_state=True, \n",
    "            activity_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "        )(inputs)\n",
    "    else:\n",
    "        _, _, encoder = layers.LSTM(\n",
    "            units=32, \n",
    "            recurrent_dropout=0.0, \n",
    "            return_sequences=False, \n",
    "            return_state=True, \n",
    "            activity_regularizer=tf.keras.regularizers.L2(0.01)\n",
    "        )(inputs)\n",
    "        \n",
    "    temp_input = layers.Input(shape=(1,))\n",
    "    temp = layers.GaussianNoise(stddev=1.0)(temp_input)\n",
    "    temp = layers.BatchNormalization()(temp)\n",
    "\n",
    "    output = layers.Concatenate()([encoder, temp])\n",
    "    output = layers.Dense(units=1, activation=None, use_bias=False, activity_regularizer=tf.keras.regularizers.L2(0.01))(output)\n",
    "\n",
    "    model = Model([inputs, temp_input], output)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d9b70d62",
   "metadata": {},
   "source": [
    "version = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ef7dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING GRU, k_idx: 0\n",
      "Epoch 1/20\n",
      "385/385 [==============================] - 2s 2ms/step - loss: 0.2748\n",
      "Epoch 2/20\n",
      "385/385 [==============================] - 1s 2ms/step - loss: 0.1202\n",
      "Epoch 3/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.1032\n",
      "Epoch 4/20\n",
      "385/385 [==============================] - 1s 2ms/step - loss: 0.0940\n",
      "Epoch 5/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0884\n",
      "Epoch 6/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0848\n",
      "Epoch 7/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0816\n",
      "Epoch 8/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0786\n",
      "Epoch 9/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0750\n",
      "Epoch 10/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0715\n",
      "Epoch 11/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0690\n",
      "Epoch 12/20\n",
      "385/385 [==============================] - 1s 2ms/step - loss: 0.0669\n",
      "Epoch 13/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0660\n",
      "Epoch 14/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0644\n",
      "Epoch 15/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0638\n",
      "Epoch 16/20\n",
      "385/385 [==============================] - 1s 2ms/step - loss: 0.0627\n",
      "Epoch 17/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0621\n",
      "Epoch 18/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0616\n",
      "Epoch 19/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0604\n",
      "Epoch 20/20\n",
      "385/385 [==============================] - 1s 3ms/step - loss: 0.0595\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'encoded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19760\\3055563910.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[0mpred_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mencodings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'encoded' is not defined"
     ]
    }
   ],
   "source": [
    "for k_idx in range(20):\n",
    "    train_tup, test_tup, val_tup, col_params = k_fold_data_validation(\n",
    "        data=data.copy(), \n",
    "        k_idx=k_idx, \n",
    "        k_frac=k_frac, \n",
    "        m=m, \n",
    "        cols=cols, \n",
    "        t_steps=t_steps, \n",
    "        n_steps=n_steps,\n",
    "        setpoint=True,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # extract tuples\n",
    "    (sequences_train, targets_train, temps_train, temps_t_train, _) = train_tup\n",
    "    (sequences_test, targets_test, temps_test, temps_t_test, sequences_masked, _) = test_tup\n",
    "    (sequences_val, targets_val, temps_val, temps_t_val, _) = val_tup\n",
    "\n",
    "    ### TRAIN GRU\n",
    "    clear_output()\n",
    "    print('TRAINING GRU, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "    # get model name\n",
    "    model_name = 'gru_v{}_k{}'.format(version, k_idx)\n",
    "    \n",
    "    # get model\n",
    "    gru_model = get_model('gru', lr=0.001)\n",
    "    \n",
    "    # train model\n",
    "    trainHist = gru_model.fit(\n",
    "        [sequences_train, temps_train],\n",
    "        targets_train,\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        validation_split=0.0,\n",
    "    )\n",
    "    \n",
    "    # get autoregressive preds and encodings\n",
    "    preds = []\n",
    "#     encodings = []\n",
    "    \n",
    "    pred_queue = deque(maxlen=t_steps)\n",
    "    for temp in temps_test[:t_steps]:\n",
    "        pred_queue.append(temp[np.newaxis, :])\n",
    "        preds.append(temp)\n",
    "\n",
    "    for seq in sequences_masked:\n",
    "        temp = pred_queue.popleft()\n",
    "        temp_pred = gru_model.predict([seq[np.newaxis, :], temp], verbose=0)\n",
    "        pred_queue.append(temp_pred)\n",
    "        preds.append(temp_pred[0])\n",
    "#         encodings.append(encoded[0])\n",
    "\n",
    "    results = {}\n",
    "    results['test_preds'] = preds\n",
    "    results['test_encodings'] = encodings\n",
    "    # save results\n",
    "    save_path = results_path + 'gru\\\\' + model_name + '.pickle'\n",
    "    with open(save_path, 'wb') as fo:\n",
    "        pickle.dump(results, fo)\n",
    "\n",
    "    # clear output\n",
    "    clear_output()\n",
    "        \n",
    "    ### TRAIN LSTM\n",
    "    clear_output()\n",
    "    print('TRAINING LSTM, k_idx: {}'.format(k_idx))\n",
    "    \n",
    "    # get model name\n",
    "    model_name = 'lstm_v{}_k{}'.format(version, k_idx)\n",
    "    \n",
    "    # get model\n",
    "    lstm_model = get_model('lstm', lr=0.001)\n",
    "    \n",
    "    # train model\n",
    "    trainHist = lstm_model.fit(\n",
    "        [sequences_train, temps_train],\n",
    "        targets_train,\n",
    "        epochs=20,\n",
    "        batch_size=128,\n",
    "        validation_split=0.0,\n",
    "    )\n",
    "    \n",
    "    # get autoregressive preds and encodings\n",
    "    preds = []\n",
    "    encodings = []\n",
    "\n",
    "    pred_queue = deque(maxlen=t_steps)\n",
    "    for temp in temps_test[:t_steps]:\n",
    "        pred_queue.append(temp[np.newaxis, :])\n",
    "        preds.append(temp)\n",
    "\n",
    "    for seq in sequences_masked:\n",
    "        temp = pred_queue.popleft()\n",
    "        temp_pred = lstm_model.predict([seq[np.newaxis, :], temp], verbose=0)\n",
    "        pred_queue.append(temp_pred)\n",
    "        preds.append(temp_pred[0])\n",
    "        encodings.append(encoded[0])\n",
    "        \n",
    "    results = {}\n",
    "    results['test_preds'] = preds\n",
    "    results['test_encodings'] = encodings\n",
    "    # save results\n",
    "    save_path = results_path + 'lstm\\\\' + model_name + '.pickle'\n",
    "    with open(save_path, 'wb') as fo:\n",
    "        pickle.dump(results, fo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a0988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
